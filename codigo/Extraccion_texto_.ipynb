{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ],
      "metadata": {
        "id": "vEaP_kMXnPLc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM5Ip7r-BIQ2",
        "outputId": "184c7f5e-b5ed-426c-a51c-0db42f486271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Todo el contenido ha sido extraído y guardado en 'resultado_scraping.txt'.\n"
          ]
        }
      ],
      "source": [
        "def extraer_todo_a_txt(url=\"https://misutmeeple.com/2018/08/resena-sagrada/\"):\n",
        "    \"\"\"\n",
        "    Esta función extrae contenido estructurado de una reseña de juego de mesa desde el sitio misutmeeple.com.\n",
        "    Utiliza requests para obtener el HTML, BeautifulSoup para analizarlo, y guarda los datos relevantes en\n",
        "    un archivo de texto llamado 'resultado_scraping.txt'. También descarga las primeras 5 imágenes del artículo\n",
        "    en una carpeta local 'imagenes_scrapeadas'.\n",
        "\n",
        "    Contenido extraído:\n",
        "    - Título de la pestaña (tag <title>)\n",
        "    - Encabezado principal (tag <h1>)\n",
        "    - Subtítulos (h2, h3, h4)\n",
        "    - Párrafos de texto no vacíos\n",
        "    - Palabras en negrita (tag <strong>)\n",
        "    - URLs de imágenes dentro del contenido\n",
        "    - Comentarios (autor y texto)\n",
        "    - Ítems de listas (li)\n",
        "    - Leyendas de imágenes (tag <figcaption>)\n",
        "    \"\"\"\n",
        "    resultados = []  # Lista para acumular el contenido extraído\n",
        "\n",
        "    # 1. Obtener el HTML de la página\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html_content = response.text\n",
        "        resultados.append(\"✅ Página obtenida con éxito\\n\")\n",
        "    else:\n",
        "        resultados.append(f\"❌ Error al acceder a la página: {response.status_code}\\n\")\n",
        "        return  # No continuar si hubo error\n",
        "\n",
        "    # 2. Crear objeto BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    # 3. Título y encabezados\n",
        "    titulo = soup.find(\"title\").text.strip()\n",
        "    resultados.append(f\"Título de la pestaña: {titulo}\\n\")\n",
        "\n",
        "    heading = soup.find(\"h1\").text.strip()\n",
        "    resultados.append(f\"H1: {heading}\\n\")\n",
        "\n",
        "    # H2, H3, H4\n",
        "    for nivel, tag in zip([\"H2\", \"H3\", \"H4\"], [\"h2\", \"h3\", \"h4\"]):\n",
        "        encabezados = soup.find_all(tag)\n",
        "        resultados.append(f\"\\n{nivel} encontrados:\\n\")\n",
        "        for i in encabezados:\n",
        "            resultados.append(f\"- {i.text.strip()}\")\n",
        "\n",
        "    # 4. Párrafos\n",
        "    parrafos = soup.find_all(\"p\")\n",
        "    resultados.append(\"\\n\\nPárrafos:\\n\")\n",
        "    for i, parrafo in enumerate(parrafos, 1):\n",
        "        texto = parrafo.text.strip()\n",
        "        if texto:\n",
        "            resultados.append(f\"{i}. {texto}\")\n",
        "\n",
        "    # 5. Palabras importantes en negrita (<strong>)\n",
        "    strongs = soup.find_all(\"strong\")\n",
        "    resultados.append(\"\\n\\nPalabras en negrita:\\n\")\n",
        "    for i in strongs:\n",
        "        texto = i.text.strip()\n",
        "        if texto:\n",
        "            resultados.append(f\"- {texto}\")\n",
        "\n",
        "    # 6. Imágenes\n",
        "    resultados.append(\"\\n\\nURLs de Imágenes encontradas:\\n\")\n",
        "    imagenes = soup.find(class_=\"entry-content-wrap\").find_all(\"img\")\n",
        "    os.makedirs(\"imagenes_scrapeadas\", exist_ok=True)\n",
        "    for i, img in enumerate(imagenes, 1):\n",
        "        img_url = img.get(\"src\")\n",
        "        if img_url.startswith(\"/\"):\n",
        "            img_url = url + img_url  # Convertir URL relativa a absoluta\n",
        "        resultados.append(f\"Imagen {i}: {img_url}\")\n",
        "\n",
        "        # Descargar las primeras 5 imágenes\n",
        "        if i <= 5:\n",
        "            img_data = requests.get(img_url).content\n",
        "            with open(f\"imagenes_scrapeadas/imagen_{i}.jpg\", \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "\n",
        "    # 7. Comentarios\n",
        "    comentarios = soup.select(\".comment-body\")\n",
        "    resultados.append(\"\\n\\nComentarios:\\n\")\n",
        "    for i, comentario in enumerate(comentarios, 1):\n",
        "        autor = comentario.find(class_=\"fn\").text.strip()\n",
        "        texto = comentario.find(class_=\"comment-content\").text.strip()\n",
        "        resultados.append(f\"{i}. AUTOR: {autor}\\nCOMENTARIO: {texto}\\n\")\n",
        "\n",
        "    # 8. Listas (<li>)\n",
        "    elementos = soup.select(\".single-content\")\n",
        "    resultados.append(\"\\n\\nElementos de listas:\\n\")\n",
        "    for ul in elementos:\n",
        "        lista_li = ul.find_all(\"li\")\n",
        "        for li in lista_li:\n",
        "            resultados.append(f\"- {li.text.strip()}\")\n",
        "\n",
        "    # 9. Leyendas de imágenes (<figcaption>)\n",
        "    leyendas = soup.find_all(\"figcaption\")\n",
        "    resultados.append(\"\\n\\nLeyendas de imágenes:\\n\")\n",
        "    for i in leyendas:\n",
        "        texto = i.text.strip()\n",
        "        if texto:\n",
        "            resultados.append(f\"- {texto}\")\n",
        "\n",
        "    # 10. Guardar todo en un archivo .txt\n",
        "    with open(\"resultado_scraping.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(resultados))\n",
        "\n",
        "    print(\"✅ Todo el contenido ha sido extraído y guardado en 'resultado_scraping.txt'.\")\n",
        "\n",
        "# Ejecutar\n",
        "extraer_todo_a_txt()\n"
      ]
    }
  ]
}