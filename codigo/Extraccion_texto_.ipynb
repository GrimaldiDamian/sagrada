{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vEaP_kMXnPLc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Todo el contenido ha sido extraído y guardado correctamente.\n"
          ]
        }
      ],
      "source": [
        "def extraer_todo_a_txt(url=\"https://misutmeeple.com/2018/08/resena-sagrada/\"):\n",
        "    \"\"\"\n",
        "    Esta función extrae contenido estructurado de una reseña de juego de mesa desde el sitio misutmeeple.com.\n",
        "    Utiliza requests para obtener el HTML, BeautifulSoup para analizarlo, y guarda los datos relevantes en\n",
        "    un archivo de texto llamado 'resultado_scraping.txt'. También descarga las primeras 5 imágenes del artículo\n",
        "    en una carpeta local 'imagenes_scrapeadas' dentro de la carpeta datos/informacion.\n",
        "\n",
        "    Contenido extraído:\n",
        "    - Título de la pestaña (tag <title>)\n",
        "    - Encabezado principal (tag <h1>)\n",
        "    - Subtítulos (h2, h3, h4)\n",
        "    - Párrafos de texto no vacíos\n",
        "    - Palabras en negrita (tag <strong>)\n",
        "    - URLs de imágenes dentro del contenido\n",
        "    - Comentarios (autor y texto)\n",
        "    - Ítems de listas (li)\n",
        "    - Leyendas de imágenes (tag <figcaption>)\n",
        "    \"\"\"\n",
        "\n",
        "    ruta_salida = \"../datos/informacion\"\n",
        "    carpeta_imagenes = f\"{ruta_salida}/imagenes_scrapeadas\"\n",
        "    os.makedirs(carpeta_imagenes, exist_ok=True)\n",
        "\n",
        "    resultados = []  # Lista para acumular el contenido extraído\n",
        "\n",
        "    # 1. Obtener el HTML de la página\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html_content = response.text\n",
        "        resultados.append(\"✅ Página obtenida con éxito\\n\")\n",
        "    else:\n",
        "        resultados.append(f\"❌ Error al acceder a la página: {response.status_code}\\n\")\n",
        "        return\n",
        "\n",
        "    # 2. Crear objeto BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    # 3. Título y encabezados\n",
        "    titulo = soup.find(\"title\").text.strip()\n",
        "    resultados.append(f\"Título de la pestaña: {titulo}\\n\")\n",
        "\n",
        "    heading = soup.find(\"h1\").text.strip()\n",
        "    resultados.append(f\"H1: {heading}\\n\")\n",
        "\n",
        "    for nivel, tag in zip([\"H2\", \"H3\", \"H4\"], [\"h2\", \"h3\", \"h4\"]):\n",
        "        encabezados = soup.find_all(tag)\n",
        "        resultados.append(f\"\\n{nivel} encontrados:\\n\")\n",
        "        for i in encabezados:\n",
        "            resultados.append(f\"- {i.text.strip()}\")\n",
        "\n",
        "    # 4. Párrafos\n",
        "    parrafos = soup.find_all(\"p\")\n",
        "    resultados.append(\"\\n\\nPárrafos:\\n\")\n",
        "    for i, parrafo in enumerate(parrafos, 1):\n",
        "        texto = parrafo.text.strip()\n",
        "        if texto:\n",
        "            resultados.append(f\"{i}. {texto}\")\n",
        "\n",
        "    # 5. Palabras en negrita\n",
        "    strongs = soup.find_all(\"strong\")\n",
        "    resultados.append(\"\\n\\nPalabras en negrita:\\n\")\n",
        "    for i in strongs:\n",
        "        texto = i.text.strip()\n",
        "        if texto:\n",
        "            resultados.append(f\"- {texto}\")\n",
        "\n",
        "    # 6. Imágenes\n",
        "    resultados.append(\"\\n\\nURLs de Imágenes encontradas:\\n\")\n",
        "    imagenes = soup.find(class_=\"entry-content-wrap\").find_all(\"img\")\n",
        "    for i, img in enumerate(imagenes, 1):\n",
        "        img_url = img.get(\"src\")\n",
        "        if img_url.startswith(\"/\"):\n",
        "            img_url = url + img_url\n",
        "        resultados.append(f\"Imagen {i}: {img_url}\")\n",
        "\n",
        "        img_data = requests.get(img_url).content\n",
        "        with open(f\"{carpeta_imagenes}/imagen_{i}.jpg\", \"wb\") as f:\n",
        "            f.write(img_data)\n",
        "\n",
        "    # 7. Comentarios\n",
        "    comentarios = soup.select(\".comment-body\")\n",
        "    resultados.append(\"\\n\\nComentarios:\\n\")\n",
        "    for i, comentario in enumerate(comentarios, 1):\n",
        "        autor = comentario.find(class_=\"fn\").text.strip()\n",
        "        texto = comentario.find(class_=\"comment-content\").text.strip()\n",
        "        resultados.append(f\"{i}. AUTOR: {autor}\\nCOMENTARIO: {texto}\\n\")\n",
        "\n",
        "    # 8. Listas\n",
        "    elementos = soup.select(\".single-content\")\n",
        "    resultados.append(\"\\n\\nElementos de listas:\\n\")\n",
        "    for ul in elementos:\n",
        "        lista_li = ul.find_all(\"li\")\n",
        "        for li in lista_li:\n",
        "            resultados.append(f\"- {li.text.strip()}\")\n",
        "\n",
        "    # 9. Leyendas de imágenes\n",
        "    leyendas = soup.find_all(\"figcaption\")\n",
        "    resultados.append(\"\\n\\nLeyendas de imágenes:\\n\")\n",
        "    for i in leyendas:\n",
        "        texto = i.text.strip()\n",
        "        if texto:\n",
        "            resultados.append(f\"- {texto}\")\n",
        "\n",
        "    # 10. Guardar todo en un archivo .txt\n",
        "    with open(f\"{ruta_salida}/resultado_scraping.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(resultados))\n",
        "\n",
        "    print(\"✅ Todo el contenido ha sido extraído y guardado correctamente.\")\n",
        "\n",
        "# Ejecutar\n",
        "extraer_todo_a_txt()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "entorno_nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
